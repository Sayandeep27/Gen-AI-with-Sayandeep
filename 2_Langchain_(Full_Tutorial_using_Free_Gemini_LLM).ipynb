{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPs8bnJkW8fFVQfmwVnkhND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayandeep27/Gen-AI-with-Sayandeep/blob/main/2_Langchain_(Full_Tutorial_using_Free_Gemini_LLM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pg7mkL_uaf9V"
      },
      "outputs": [],
      "source": [
        "api=\"AIzaSyBpijVSmuqyt5qelKWaPKpd-Ys30wTd37w\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVImFR1rixIq",
        "outputId": "9c56a1a6-3fa4-4c0a-eb65-8d5cfdd6e89c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.49 (from langchain-google-genai)\n",
            "  Downloading langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.10.6)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.47 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.47)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (0.3.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.47->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "api_key = api  # Get your free API key from https://ai.google.dev/\n",
        "\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", google_api_key=api_key, temperature=0.1)"
      ],
      "metadata": {
        "id": "qXcGt90vjBTJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"What is the capital of England?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qYeRY7jVjHE_",
        "outputId": "1931165b-2bf2-4408-c6ee-a39613a07dc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'London\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0lEPdCEfD36z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt** **Templates**"
      ],
      "metadata": {
        "id": "QfvFo4MAEH8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import template\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template=PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"Hello {name}, how can I assist you today?\"\n",
        ")\n",
        "\n",
        "\n",
        "formatted_prompt=template.format(name=\"Sayandeep\")"
      ],
      "metadata": {
        "id": "o96rpy1oENWe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVzlqLMyEp2U",
        "outputId": "07acbfbe-1291-4733-db0c-849f21d79883"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Sayandeep, how can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Multiple Variables\n",
        "\n",
        "template=PromptTemplate(\n",
        "    input_variables=[\"greet\",\"name\",],\n",
        "    template=\"{greet} !! Hello {name}, how can I assist you today?\"\n",
        ")\n",
        "\n",
        "\n",
        "formatted_prompt=template.format(greet=\"Good Morning\",name=\"Sayandeep\")\n",
        "\n",
        "print(formatted_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSHwlYK3Er4m",
        "outputId": "e7b1aa97-a708-4f23-bd57-f5fac8b49043"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good Morning !! Hello Sayandeep, how can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Few Shot Prompt Template**"
      ],
      "metadata": {
        "id": "i5iAf1xuHCeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few shot prompting\n",
        "'''\n",
        "What is Few-Shot Prompting?\n",
        "Few-shot prompting is a technique where you give an AI model a few examples of what you want before asking it to complete a task. This helps the model understand the pattern and generate better responses.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NNiGZQbKEzPe",
        "outputId": "84a250d6-8326-4643-afab-5857fb6f110e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWhat is Few-Shot Prompting?\\nFew-shot prompting is a technique where you give an AI model a few examples of what you want before asking it to complete a task. This helps the model understand the pattern and generate better responses.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "# Few-shot examples\n",
        "examples = [\n",
        "    {\"input\": \"The product is amazing! I love it.\", \"output\": \"Positive\"},\n",
        "    {\"input\": \"It was a complete waste of money. I regret buying it.\", \"output\": \"Negative\"},\n",
        "]\n",
        "\n",
        "# Example formatter\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Review: {input}\\nSentiment: {output}\\n\"\n",
        ")\n",
        "\n",
        "# Few-Shot Prompt Template\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Classify the sentiment of the following reviews:\\n\",\n",
        "    suffix=\"Review: {input}\\nSentiment:\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "# Test Few-Shot Prompting\n",
        "new_input = {\"input\": \"The quality is decent, but I expected better.\"}\n",
        "formatted_prompt = few_shot_prompt.format(**new_input)"
      ],
      "metadata": {
        "id": "fC5miSaeGeFb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UsC8Sa-G96y",
        "outputId": "08ca6ffa-1c99-4b70-8ce6-df5f22fb9e5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classify the sentiment of the following reviews:\n",
            "\n",
            "\n",
            "Review: The product is amazing! I love it.\n",
            "Sentiment: Positive\n",
            "\n",
            "\n",
            "Review: It was a complete waste of money. I regret buying it.\n",
            "Sentiment: Negative\n",
            "\n",
            "\n",
            "Review: The quality is decent, but I expected better.\n",
            "Sentiment:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get AI response\n",
        "\n",
        "response=llm.invoke(formatted_prompt)"
      ],
      "metadata": {
        "id": "wtlRu3NhG_Zi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(f\"Prompt:\\n{formatted_prompt}\")\n",
        "print(f\"\\nAI Response:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fchrQuklHURu",
        "outputId": "6dc3de79-affb-48e6-f446-0fac22ac025b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "Classify the sentiment of the following reviews:\n",
            "\n",
            "\n",
            "Review: The product is amazing! I love it.\n",
            "Sentiment: Positive\n",
            "\n",
            "\n",
            "Review: It was a complete waste of money. I regret buying it.\n",
            "Sentiment: Negative\n",
            "\n",
            "\n",
            "Review: The quality is decent, but I expected better.\n",
            "Sentiment:\n",
            "\n",
            "AI Response:\n",
            "Neutral (or slightly negative)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Few-Shot Prompting with Chat Models (ChatPromptTemplate)**"
      ],
      "metadata": {
        "id": "p4DitoJEH7Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Define system and human message templates\n",
        "system_message = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful AI assistant that classifies text sentiment.\"\n",
        ")\n",
        "human_message = HumanMessagePromptTemplate.from_template(\n",
        "    \"Review: {review}\\nSentiment:\"\n",
        ")\n",
        "\n",
        "# Create Few-Shot ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
        "\n",
        "formatted_prompt = chat_prompt.format(review=\"I don't love this product! It's bad\")\n",
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsRd5zWDHWLS",
        "outputId": "afdabf97-716f-465d-f0ce-995c19e656b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: You are a helpful AI assistant that classifies text sentiment.\n",
            "Human: Review: I don't love this product! It's bad\n",
            "Sentiment:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "X4ihsyelH_rr",
        "outputId": "2c178ae2-9613-4594-b2e2-18c91220e742"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentiment: Negative\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6gkyrRAIJTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output Parsers in LangChain**\n",
        "Output Parsers in LangChain help convert the raw output from a language model into structured data. They are useful when you need the output in a specific format, such as JSON, a list, or key-value pairs.\n",
        "\n"
      ],
      "metadata": {
        "id": "-gk325YfIvWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define an output parser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List three programming languages, separated by commas.\",\n",
        "    input_variables=[],\n",
        "    output_parser=output_parser\n",
        ")\n",
        "\n",
        "# Format the prompt\n",
        "formatted_prompt = prompt.format()\n",
        "print(formatted_prompt)\n",
        "\n",
        "# Parse the output\n",
        "parsed_output = output_parser.parse(\"Python, JavaScript, C++\")\n",
        "print(parsed_output)\n",
        "\n",
        "\n",
        "#The model's response is converted into a structured list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKgkL6PoI2TF",
        "outputId": "f0a4bc09-ffcf-4941-f4e8-0b0df347efd8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List three programming languages, separated by commas.\n",
            "['Python', 'JavaScript', 'C++']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "parsed_output = output_parser.parse(\"Red, Green, Blue\")\n",
        "print(parsed_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXHNDuAzI8AV",
        "outputId": "12ae0c1d-f96c-4b58-e096-7e08950c2685"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Red', 'Green', 'Blue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoAzljlWJFft"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Document Loaders in LangChain**\n",
        "Document Loaders in LangChain help in loading text from various sources like PDFs, Word files, Notion, web pages, databases, and cloud storage. They are essential when working with Retrieval-Augmented Generation (RAG) and other NLP tasks."
      ],
      "metadata": {
        "id": "u5QlThoWJ4Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEhKZkKGJK1p",
        "outputId": "6ccba092-d468-4c88-e448-f4911112fc69"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.47)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.18)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load a PDF\n",
        "loader = PyPDFLoader(\"/content/Sayandeep_Resume.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "# Print the first page content\n",
        "print(documents[0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNV9fWGsJ9bV",
        "outputId": "1667543a-2fd2-4585-86e4-ee95978d287d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sayandeep Sarkar +91- 7001932512\n",
            "Bachelor of Engineering sarkarsayandeep093@gmail.com\n",
            "in Mechanical Engineering GitHub\n",
            "Jadavpur University, Kolkata, India LinkedIn\n",
            "Summary\n",
            "Currently a final year Undergrad at Jadavpur University, highly interested in Data Engineering, Data Science and\n",
            "Software Development\n",
            "SPECIALIST -Codeforces (1400*) (solved 350+ questions)\n",
            "Solved 550+ Data Structures and Algorithms questions on Leetcode and overall 1000+ questions including Codeforces\n",
            "and Gfg\n",
            "Internship\n",
            "Data Engineering Intern atIIM Shillong\n",
            "Developed and maintained Data Engineering pipelines usingApache Airflow, ensuring efficient scheduling and\n",
            "monitoring ofETL tasks. Worked extensively withApache Kafkato build real-time data streaming solutions,\n",
            "enabling seamless data integration and processing. LeveragedPySpark for big data processing, optimizing\n",
            "performance and scalability in large-scale data transformation tasks.\n",
            "Roles and Responsibilities\n",
            "Central Placement Coordinatorof Jadavpur University for 2025 batch\n",
            "Education\n",
            "Degree Institute Board / UniversityCGPA/Percentage Year\n",
            "BE Mechanical Engineering Jadavpur University, Kolkata - 7.5 2021-2025\n",
            "Senior Secondary Falakata High School WBCHSE 95.40% 2020\n",
            "Matriculation Falakata High School WBCHSE 95.14% 2018\n",
            "PROJECTS\n",
            "• Epiphany\n",
            "Python,Google GenAI, Pypdf2, Langchain, FAISS, Flask\n",
            "– Designed and developed an intelligent web application with interactive features, including a YouTube video summarizer and\n",
            "a PDF-based conversational assistant. Leveraged Google Generative AIfor content generation andFAISS for advanced\n",
            "vector-based search, enabling context-driven question answering. Built robust PDF text extraction, segmentation, and\n",
            "storage processes to deliver seamless user interactions and efficient information retrieval.Github Link\n",
            "• Prediction of Concrete Compressive Strength Using Machine Learning\n",
            "Python, Flask, Jupyter Jadavpur University\n",
            "– Predicts the Concrete compressive strength(MPa, megapascals) considering cement blastFurnace flyAsh courseAggregate\n",
            "age using various ML algorithms likeLinear regression, Random forest, XGBRegressoretc with best possible accuracy of\n",
            "approx 91 Percent.Also did EDA and usedstandard scalarfunction for standardization Github Link\n",
            "• Advanced Forecasting of Energy Consumption Using Machine Learning and Time Series Feature Engineering\n",
            "Python,Xgboost,ARIMA,Google Colab\n",
            "– Developed a time series forecasting model for energy consumption using XGBoost. This project included data preparation,\n",
            "outlier detection, and splitting data with TimeSeriesSplit for cross-validation. Engineered time-based features and lag\n",
            "variables to capture patterns. The XGBoost model was trained and evaluated using RMSE, then retrained on the entire\n",
            "dataset to forecast future energy use, with predictions visualized for accuracy assessmentGithub Link\n",
            "Technical Skills\n",
            "• Programming Languages: C/C++ , Python, Javascript\n",
            "• Tools and Frameworks: Jupyter, Flask, Databricks, Pyspark, Docker, Apache Kafka,Langchain, GenAI , Snowflake, Visual\n",
            "Studio , Git , Github & Pycharm\n",
            "• Ohers: Machine Learning, Deep Learning, SQL, DBMS ,OOPS, Operating system,Data Warehousing, Computer Networks\n",
            "• Math courses: Industrial Statistics (Descriptive and Inferential), Probability, Linear Algebra, Calculus\n",
            "Coding Profiles\n",
            "• Codeforces Specialist (1400*)\n",
            "• Leetcode Solved 550 + DSA Questions\n",
            "• Codechef 3 star (1702)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTjU55ABKLhM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Splitters in LangChain**\n",
        "Text Splitters in LangChain help break large documents into smaller chunks. This is useful for:\n",
        " * Efficient retrieval in RAG (Retrieval-Augmented Generation)\n",
        " * Chunking text for embeddings in vector databases\n",
        " * Processing large documents without exceeding token limits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h7ZoD4J4LYB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-based Text Splitter\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text = \"\"\"Artificial Intelligence (AI) is transforming industries worldwide.\n",
        "From healthcare to finance, AI is improving efficiency and decision-making.\"\"\"\n",
        "\n",
        "# Initialize splitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=5, chunk_overlap=2)\n",
        "\n",
        "# Split text\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# Print chunks\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St_ebwsQKooy",
        "outputId": "416c5142-c439-44f9-dc83-267ea67caa3e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Artificial Intelligence (AI) is transforming industries worldwide. \n",
            "From healthcare to finance, AI is improving efficiency and decision-making.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive Character Splitter (Better for Documents)\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R2f1tt4Lonw",
        "outputId": "8270c4ec-8320-41a2-9375-44f7ac460444"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Artificial Intelligence (AI) is transforming\n",
            "\n",
            "Chunk 2:\n",
            "industries worldwide.\n",
            "\n",
            "Chunk 3:\n",
            "From healthcare to finance, AI is improving\n",
            "\n",
            "Chunk 4:\n",
            "improving efficiency and decision-making.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IRY73T3aMMiB"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chains in LangChain using LCEL (LangChain Expression Language)**\n",
        "Chains in LangChain allow you to connect multiple components (LLMs, retrievers, memory, etc.) into a single pipeline.\n",
        "\n",
        "* LCEL (LangChain Expression Language) makes defining chains more intuitive using a functional programming approach.\n",
        "* No need for complex classes—just chain components together like a function!"
      ],
      "metadata": {
        "id": "16LzuO1vMfjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define a prompt\n",
        "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
        "\n",
        "# Define an LLM\n",
        "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", google_api_key=api_key, temperature=0.1)\n",
        "\n",
        "# Create the LCEL chain\n",
        "chain = prompt | llm\n",
        "\n",
        "\n",
        "# Run the chain\n",
        "\n",
        "response=chain.invoke({\"country\":\"France\"})\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xt1IYZaMRht",
        "outputId": "be767965-79c1-4061-96ff-d70b2053922f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEcFArceNYx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LCEL Chain with Memory (Conversation Chain)**"
      ],
      "metadata": {
        "id": "plzwSCFPNqFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Create memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Define a prompt\n",
        "prompt = PromptTemplate.from_template(\"Chat history: {chat_history}\\nUser: {input}\\nAI:\")\n",
        "\n",
        "# LCEL Chain: Memory → Prompt → LLM\n",
        "chain = (\n",
        "    {\"chat_history\": memory.load_memory_variables | RunnablePassthrough(), \"input\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "response = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(response)\n",
        "\n",
        "# Save memory\n",
        "memory.save_context({\"input\": \"Hello!\"}, {\"output\": response})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_mWj5YQNtNx",
        "outputId": "9b6624a8-2d2b-47c5-ff8c-88183d4532e4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-467f2f7a1b3b>:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KzhvUxBrNvZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Memory in LangChain**\n",
        "Memory in LangChain allows stateful interactions by storing past conversations or information.\n",
        "* Keeps chat history for context\n",
        "* Improves user experience in chatbots\n",
        "* Works with various storage backends"
      ],
      "metadata": {
        "id": "oH_MNskENz3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Initialize Google Gemini Model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", google_api_key=api, temperature=0.1)\n",
        "\n",
        "# Create Memory to Store Conversation History\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create Conversation Chain with Memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "\n",
        "# Simulate a conversation\n",
        "print(conversation.predict(input=\"Hello, who are you?\"))\n",
        "print(conversation.predict(input=\"Can you remember my name is Alex?\"))\n",
        "print(conversation.predict(input=\"What is my name?\"))  # Should remember \"Alex\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo6IC4CoOoRb",
        "outputId": "f1b617d7-ad64-480b-cd88-27567fb6ee40"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am a large language model, trained by Google.  I don't have a name, per se, but you can call me AI if you like.  I exist as a computer program and have been trained on a massive dataset of text and code. This dataset included a wide range of information, from books and articles to websites and code repositories.  Because of this training, I can communicate in response to a wide range of prompts and questions, generating different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. I can even translate languages!  I don't have personal experiences or emotions like humans do, and my knowledge is based on the data I was trained on, which has a cutoff point.  So, for example, I wouldn't know about current events past my last training update.  Is there anything you'd like to know more about?\n",
            "As a large language model, I don't have memory of past conversations. Each interaction we have starts fresh.  So, while I can't remember your name from a previous interaction, I can certainly use it if you tell me. You said your name is Alex, and I will refer to you as Alex in this conversation.  Is there anything else I can help you with, Alex?\n",
            "Your name is Alex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nji__a87Py2p"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5H2DFtIQJ9w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}